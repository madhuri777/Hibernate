<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Red Hat Data Grid on Three Clouds (the details behind the demo)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CVkkT6u3OR0/" /><category term="Apache OpenWhisk" scheme="searchisko:content:tags" /><category term="cloud" scheme="searchisko:content:tags" /><category term="Cloud Services" scheme="searchisko:content:tags" /><category term="Data Grid" scheme="searchisko:content:tags" /><category term="Demo" scheme="searchisko:content:tags" /><category term="Eclipse Vert.x" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="gluster" scheme="searchisko:content:tags" /><category term="JBoss Data Grid" scheme="searchisko:content:tags" /><category term="OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="Red Hat JBoss Data Grid" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><category term="Red Hat Summit 2018" scheme="searchisko:content:tags" /><category term="vert.x" scheme="searchisko:content:tags" /><author><name>Sebastian Łaskawiec</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_data_grid_on_three_clouds_the_details_behind_the_demo</id><updated>2018-06-19T11:00:57Z</updated><published>2018-06-19T11:00:57Z</published><content type="html">&lt;p&gt;If you saw or heard about the &lt;a href="https://developers.redhat.com/blog/2018/05/10/red-hat-summit-2018-burr-sutter-demo/"&gt;multi-cloud demo at Red Hat Summit 2018&lt;/a&gt;, this article details how we ran &lt;a href="https://developers.redhat.com/products/datagrid/overview/"&gt;Red Hat Data Grid&lt;/a&gt; in active-active-active mode across three cloud providers. This set up enabled us to show a fail over between cloud providers in real time with no loss of data. In addition to Red Hat Data Grid, we used &lt;a href="https://developers.redhat.com/blog/2018/03/13/eclipse-vertx-first-application/"&gt;Vert.x&lt;/a&gt; (reactive programming), &lt;a href="https://developers.redhat.com/blog/category/topics/serverless/"&gt;OpenWhisk&lt;/a&gt; (serverless), and &lt;a href="https://www.redhat.com/en/technologies/storage/gluster"&gt;Red Hat Gluster Storage&lt;/a&gt; (software-defined storage.)&lt;/p&gt; &lt;p&gt;This year’s &lt;a href="https://developers.redhat.com/blog/tag/red-hat-summit-2018/"&gt;Red Hat Summit&lt;/a&gt; was quite an adventure for all of us. A trip to San Francisco is probably on the bucket list of IT geeks from all over the world. Also, we were able to meet many other Red Hatters, who work remotely for Red Hat as we do.  However, the best part was that we had something important to say: “we believe in the hybrid/multi cloud&amp;#8221; and we got to prove that live on stage.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-499827 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image5-2-1024x685.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image5-2-1024x685.png" alt="" width="640" height="428" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image5-2-1024x685.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image5-2-300x201.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image5-2-768x514.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image5-2.png 1299w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;address style="text-align: center;"&gt;&lt;em&gt;Photo credit: Bolesław Dawidowicz&lt;/em&gt;&lt;/address&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;span id="more-499757"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h3&gt;Our keynote demo&lt;/h3&gt; &lt;p&gt;In one of the earlier keynote demos, our hybrid cloud was provisioned live in a rack on stage with Red Hat OpenShift. It was quite impressive to see all the tools working together to make the demo succeed. Together with a group of teammates, we were responsible for creating the fourth keynote demo, which involved interesting middleware technologies such as Red Hat Data Grid, Vert.x, OpenWhisk, and Red Hat Gluster.  You watch the demo online if you haven&amp;#8217;t already seen it:&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='390' src='https://www.youtube.com/embed/hu2BmE1Wk_Q?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;p&gt;What you couldn’t see is that we had a maintenance team sitting behind the scenes, observing whether everything was fine (below, Marek Posolda from the Red Hat SSO team and Galder Zamarreño from the Red Hat Data Grid team):&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-499797 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image6-1024x769.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image6-1024x769.png" alt="" width="640" height="481" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image6-1024x769.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image6-300x225.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image6-768x577.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image6.png 1241w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;The demo went well, so it&amp;#8217;s time to explain how we did it.&lt;/p&gt; &lt;h3&gt;Keynote Demo #4 Architecture&lt;/h3&gt; &lt;p&gt;We were using three data centers (Amazon, Azure, and the on-stage rack) that were working in an active-active-active replication configuration.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-499777 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image2-1024x575.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image2-1024x575.png" alt="" width="640" height="359" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image2-1024x575.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image2-300x168.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image2-768x431.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image2.png 1252w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;p&gt;Each site was using a microservices and serverless architecture. We also used two storage layers: Gluster for storing images and Red Hat Data Grid for storing metadata (in JSON format):&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-499767 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image1-1024x472.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image1-1024x472.png" alt="" width="640" height="295" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image1-1024x472.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image1-300x138.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image1-768x354.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image1.png 1264w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/p&gt; &lt;h3&gt;Data Grid Setup for Cross-site Replication&lt;/h3&gt; &lt;p&gt;The active-active-active setup for Red Hat Data Grid was implemented using &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_data_grid/7.2/html/administration_and_configuration_guide/set_up_cross_datacenter_replication"&gt;Cross-site replication&lt;/a&gt; (also known as x-site replication) feature. However, we had to slightly adjust the configuration:&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-499787 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image3.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image3.png" alt="" width="996" height="341" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/image3.png 996w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image3-300x103.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/image3-768x263.png 768w" sizes="(max-width: 996px) 100vw, 996px" /&gt;&lt;/p&gt; &lt;p&gt;The sites communicated with each other using Red Hat OpenShift’s load balancer services. The load balancers were allocated up front and their coordinates (all three clouds) were injected into a secret, creating a so-called global cluster discovery string. The configuration XML file was put into a &lt;code&gt;ConfigMap&lt;/code&gt; allowing us to easily update the configuration manually from the UI (if needed).&lt;/p&gt; &lt;p&gt;Since we strongly believe in automation, we created automated setup scripts for provisioning Red Hat Data Grid to all three clouds. The scripts can be found on our &lt;a href="https://github.com/rhdemo/jdg-as-a-service"&gt;GitHub repository&lt;/a&gt;. We also provided a customized version of the scripts that can simulate x-site replication using three different projects. You might start the Red Hat OpenShift Container Platform local instance using the &lt;code&gt;oc cluster up&lt;/code&gt; command and then run &lt;a href="https://github.com/rhdemo/jdg-as-a-service/blob/master/local-full-deployment.sh"&gt;this provisioning script&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now, let’s have a look at all the configuration bits and explore how they work together (you may check them using the local provisioning script). Let’s start with the global cluster discovery secret:&lt;/p&gt; &lt;p&gt;View the code on &lt;a href="https://gist.github.com/slaskawi/91ff7aff0584f5ebb52b3a2314db46ef"&gt;Gist&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;DISCOVERY&lt;/code&gt; string is specific to &lt;a href="http://www.jgroups.org/manual4/index.html#TCPPING_Prot"&gt;TCPPING&lt;/a&gt;, which was used for global cluster discovery. The next parameter is &lt;code&gt;EXT_ADDR&lt;/code&gt;, which represents the load balancer&amp;#8217;s public IP address. The JGroups communication toolkit needs to bind the global cluster transport to that particular address. &lt;code&gt;SITE&lt;/code&gt; describes the local site: is it Amazon or Azure or is it the private (on-stage rack) one?&lt;/p&gt; &lt;p&gt;The next piece of the puzzle is configuration. The full XML file can be found &lt;a href="https://gist.github.com/slaskawi/ca522cec11c4019b101afb9cbef5e2dd"&gt;here&lt;/a&gt;. In this blog post, we will focus only on the crucial bits:&lt;/p&gt; &lt;p&gt;View the code on &lt;a href="https://gist.github.com/slaskawi/54a7b231d69cf0f953e38baa14f3f368"&gt;Gist&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The local cluster (inside a single data center) discovers all other members using the &lt;a href="https://github.com/jgroups-extras/jgroups-kubernetes"&gt;KUBE_PING&lt;/a&gt; protocol. All the messages are forwarded to other sites using the &lt;a href="http://www.jgroups.org/manual4/index.html#RELAY2"&gt;RELAY2&lt;/a&gt; protocol. The value of &lt;code&gt;jboss.relay.site&lt;/code&gt; is injected into environmental variables using the &lt;a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/"&gt;Downward API&lt;/a&gt; from the &lt;code&gt;jdg-app&lt;/code&gt; secret. The last interesting bit is that &lt;code&gt;max_site_masters&lt;/code&gt; is set to &lt;code&gt;1000&lt;/code&gt;. Since we do not know how many instances there are behind the load balancers, we need to ensure that each one of them can act as a site master and forward all the traffic to the other site.&lt;/p&gt; &lt;p&gt;The next stack is defined for the &lt;code&gt;RELAY&lt;/code&gt; protocol:&lt;/p&gt; &lt;p&gt;View the code on &lt;a href="https://gist.github.com/slaskawi/7483ef2915bf53e2cf2abad080c5f5ab"&gt;Gist&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Starting from the TCP protocol, we need to ensure that we bind into the load balancer public address. That’s why all of the load balancers have to be provisioned up front. Again, we inject this variable from the &lt;code&gt;jdg-app&lt;/code&gt; secret. Another interesting piece is &lt;code&gt;TCPPING&lt;/code&gt;, where we use the global cluster discovery string (from the &lt;code&gt;jdg-app&lt;/code&gt; secret). The &lt;code&gt;FD_ALL&lt;/code&gt; timeout is set to a quite high number, since we would like to tolerate short site downtimes.&lt;/p&gt; &lt;p&gt;View the code on &lt;a href="https://gist.github.com/slaskawi/35c57399c89168e60dbcd2a96bc8baf9"&gt;Gist&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Each site configured all three sites as async backups. This allows achieving an active-active-active kind of setup.&lt;/p&gt; &lt;h3&gt;Final Thoughts&lt;/h3&gt; &lt;p&gt;An active-active-active setup is a very interesting approach for handling a large amount of globally routed traffic. However, a few things need to be considered before implementing it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do not write the same key from different data centers. If you do, you may get into trouble very quickly due to network delays.&lt;/li&gt; &lt;li&gt;When the site goes down and comes back up again, you must trigger a state transfer to synchronize it. This is a manual task, but we are working hard to making it fully transparent.&lt;/li&gt; &lt;li&gt;Make sure your application can tolerate asynchronous replication. Cross-site replication with synchronous caches might be problematic.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We learned a lot about globally load-balanced setups during the demo work. Discovering new use cases was quite fun and we believe we know how to make Red Hat Data Grid even better for these kinds of scenarios.&lt;/p&gt; &lt;p&gt;Have fun with the cross-site replication and see you at the next Summit!&lt;br /&gt; The Red Hat Data Grid Team&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F19%2Fred-hat-data-grid-on-three-clouds%2F&amp;#38;linkname=Red%20Hat%20Data%20Grid%20on%20Three%20Clouds%20%28the%20details%20behind%20the%20demo%29" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F19%2Fred-hat-data-grid-on-three-clouds%2F&amp;#38;linkname=Red%20Hat%20Data%20Grid%20on%20Three%20Clouds%20%28the%20details%20behind%20the%20demo%29" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F19%2Fred-hat-data-grid-on-three-clouds%2F&amp;#38;linkname=Red%20Hat%20Data%20Grid%20on%20Three%20Clouds%20%28the%20details%20behind%20the%20demo%29" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F19%2Fred-hat-data-grid-on-three-clouds%2F&amp;#38;linkname=Red%20Hat%20Data%20Grid%20on%20Three%20Clouds%20%28the%20details%20behind%20the%20demo%29" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F19%2Fred-hat-data-grid-on-three-clouds%2F&amp;#38;linkname=Red%20Hat%20Data%20Grid%20on%20Three%20Clouds%20%28the%20details%20behind%20the%20demo%29" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F19%2Fred-hat-data-grid-on-three-clouds%2F&amp;#38;linkname=Red%20Hat%20Data%20Grid%20on%20Three%20Clouds%20%28the%20details%20behind%20the%20demo%29" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F19%2Fred-hat-data-grid-on-three-clouds%2F&amp;#38;linkname=Red%20Hat%20Data%20Grid%20on%20Three%20Clouds%20%28the%20details%20behind%20the%20demo%29" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F19%2Fred-hat-data-grid-on-three-clouds%2F&amp;#38;linkname=Red%20Hat%20Data%20Grid%20on%20Three%20Clouds%20%28the%20details%20behind%20the%20demo%29" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F19%2Fred-hat-data-grid-on-three-clouds%2F&amp;#38;title=Red%20Hat%20Data%20Grid%20on%20Three%20Clouds%20%28the%20details%20behind%20the%20demo%29" data-a2a-url="https://developers.redhat.com/blog/2018/06/19/red-hat-data-grid-on-three-clouds/" data-a2a-title="Red Hat Data Grid on Three Clouds (the details behind the demo)"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/19/red-hat-data-grid-on-three-clouds/"&gt;Red Hat Data Grid on Three Clouds (the details behind the demo)&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CVkkT6u3OR0" height="1" width="1" alt=""/&gt;</content><summary>If you saw or heard about the multi-cloud demo at Red Hat Summit 2018, this article details how we ran Red Hat Data Grid in active-active-active mode across three cloud providers. This set up enabled us to show a fail over between cloud providers in real time with no loss of data. In addition to Red Hat Data Grid, we used Vert.x (reactive programming), OpenWhisk (serverless), and Red Hat Gluster S...</summary><dc:creator>Sebastian Łaskawiec</dc:creator><dc:date>2018-06-19T11:00:57Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/19/red-hat-data-grid-on-three-clouds/</feedburner:origLink></entry><entry><title>Official Apache Camel branded swags available for sale</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/7BRVtyIuTpE/official-apache-camel-branded-swags.html" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="feed_group_name_fusesource" scheme="searchisko:content:tags" /><category term="feed_name_clausibsen" scheme="searchisko:content:tags" /><author><name>Claus Ibsen</name></author><id>searchisko:content:id:jbossorg_blog-official_apache_camel_branded_swags_available_for_sale</id><updated>2018-06-18T07:39:36Z</updated><published>2018-06-18T07:39:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;Hi Cameleers,&lt;br /&gt;&lt;br /&gt;with great joy I'm happy to announce that Camel branded swag:&lt;br /&gt;T-Shirts, Cases, Posters, Mugs, Notebooks and a whole lot more, is&lt;br /&gt;available at the official Apache Software Foundation store at Redbubble.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://www.redbubble.com/people/comdev/works/32232604-apache-camel?asc=u"&gt;&lt;img border="0" data-original-height="425" data-original-width="678" height="200" src="https://2.bp.blogspot.com/-AxKCyGXa8cE/Wydgokc-iqI/AAAAAAAABos/OFfKX7oZg2EpC-GFQJT_nl89ZJzsnmpwwCEwYBhgL/s320/swag-store.png" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;The store is here:&lt;br /&gt;&lt;a href="https://www.redbubble.com/people/comdev/works/32232604-apache-camel?asc=u"&gt;https://www.redbubble.com/people/comdev/works/32232604-apache-camel?asc=u&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;From any purchases you make a percentage goes as a donation to the ASF.&lt;br /&gt;&lt;br /&gt;Thanks to Zoran Regvart whom helped us get the Camel swags into the Apache store.&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=Wk7xgi0KwBY:4mScTUiLT_Q:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=Wk7xgi0KwBY:4mScTUiLT_Q:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=Wk7xgi0KwBY:4mScTUiLT_Q:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=Wk7xgi0KwBY:4mScTUiLT_Q:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=Wk7xgi0KwBY:4mScTUiLT_Q:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=Wk7xgi0KwBY:4mScTUiLT_Q:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=Wk7xgi0KwBY:4mScTUiLT_Q:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/ApacheCamel/~4/Wk7xgi0KwBY" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/7BRVtyIuTpE" height="1" width="1" alt=""/&gt;</content><summary>Hi Cameleers, with great joy I'm happy to announce that Camel branded swag: T-Shirts, Cases, Posters, Mugs, Notebooks and a whole lot more, is available at the official Apache Software Foundation store at Redbubble. The store is here: https://www.redbubble.com/people/comdev/works/32232604-apache-camel?asc=u From any purchases you make a percentage goes as a donation to the ASF. Thanks to Zoran Reg...</summary><dc:creator>Claus Ibsen</dc:creator><dc:date>2018-06-18T07:39:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/Wk7xgi0KwBY/official-apache-camel-branded-swags.html</feedburner:origLink></entry><entry><title>Red Hat Single Sign-On in Keynote demo on Red Hat Summit!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/keYdteRKPD4/red-hat-single-sign-on-in-keynote-demo.html" /><category term="feed_group_name_keycloak" scheme="searchisko:content:tags" /><category term="feed_name_keycloak" scheme="searchisko:content:tags" /><author><name>Marek Posolda</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_single_sign_on_in_keynote_demo_on_red_hat_summit</id><updated>2018-06-18T06:59:53Z</updated><published>2018-06-18T06:59:00Z</published><content type="html">&lt;p&gt;Red Hat Summit is one of the most important events during the year. Many geeks, Red Hat employees and customers have great opportunity to meet, learn new things and attend lots of interesting presentations and trainings. During the summit this year, there were few breakout sessions, which were solely about Keycloak and Red Hat SSO. You can take a look at &lt;a href="http://blog.keycloak.org/2018/05/red-hat-single-sign-on-red-hat-summit.html"&gt;this blogpost&lt;/a&gt; for more details. &lt;p&gt;One of the most important parts of Red Hat Summit are Keynote demos, which show the main bullet points and strategies going forward. Typically they also contain the demos of the most interesting technologies, which Red Hat uses. &lt;p&gt;On the Thursday morning keynote, there was &lt;a href="https://www.youtube.com/watch?v=hu2BmE1Wk_Q&amp;feature=youtu.be&amp;t=385"&gt;this demo&lt;/a&gt; to show the Hybrid Cloud with 3 clouds (Azure, Amazon, Private) in action! There were many technologies and interesting projects involved. Among others, let's name &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/data-grid"&gt;Red Hat JBoss Data Grid (JDG)&lt;/a&gt;, &lt;a href="https://openwhisk.apache.org/"&gt;OpenWhisk&lt;/a&gt; or &lt;a href="https://www.gluster.org/"&gt;Gluster FS&lt;/a&gt;. The &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;RH-SSO&lt;/a&gt; (Red Hat product based on Keycloak project) had a honor to be used as well. &lt;h2&gt;Red Hat SSO setup details&lt;/h2&gt; &lt;p&gt;The frontend of the demo was the simple mobile game. RH-SSO was used at the very first stage to authenticate users to the mobile game. Each attendee had an opportunity to try it by yourself. In total, we had 1200 players of the game. &lt;p&gt;There was loadbalancer up-front and every user was automatically forwarded to one of the 3 clouds. The mobile application used &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.2/html/securing_applications_and_services_guide/openid_connect_3#javascript_adapter"&gt;RH-SSO Javascript adapter&lt;/a&gt; (keycloak.js) to communicate with RH-SSO. &lt;p&gt;With Javascript application, whole OpenID Connect login flow happens within browser and hence can rely on sticky session. So since Javascript adapter is used, you may think that we can do just "easy" setup and let the RH-SSO instances across all 3 clouds to be independent of each other and have each of them to use separate RDBMS and infinispan caches. See the image below for what such a setup would look like: &lt;p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://2.bp.blogspot.com/-vL-5tH6jujE/WydSVbwO65I/AAAAAAAANno/0QEoU2PQyzYBJMFAQkBiE4O6O_345JhkwCPcBGAYYCw/s1600/cross-dc-blog-architecture-rhsso.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="https://2.bp.blogspot.com/-vL-5tH6jujE/WydSVbwO65I/AAAAAAAANno/0QEoU2PQyzYBJMFAQkBiE4O6O_345JhkwCPcBGAYYCw/s1600/cross-dc-blog-architecture-rhsso.png" data-original-width="512" data-original-height="788" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;With this setup, every cloud is aware just about the users and sessions created on itself. This is fine with sticky session, but it won’t work for failover scenarios in case if one of the 3 clouds is broken/removed. There are also other issues with it - for example that admins and users see just sessions created on particular cloud. There are also potential security issues. For example when admin disables user on one cloud, user would still be enabled on other clouds as changes to user won’t be propagated to other clouds. &lt;p&gt;So we rather want to show more proper setup aware of the replication. Also because one part of the demo was showing failover in action. One of the 3 clouds (Amazon) was killed and users, who were previously logged in Amazon, were redirected to one of the remaining 2 clouds. The point was that the end user won't be able to recognize any change. Hence users previously logged in Amazon must be still able to refresh their tokens in Azure or Private cloud. This in turn meant that the data (both users, user sessions and caches) need to be aware of all 3 clouds. &lt;p&gt;In Keycloak 3.X, we added support for &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.2/html/server_installation_and_configuration_guide/operating-mode#crossdc-mode"&gt;Cross-datacenter (Cross-site) setup&lt;/a&gt; with usage of external JDG servers to replicate data among datacenters (tech preview in RH-SSO 7.2). The demo was using exactly this setup. Each site had JDG server and all 3 sites communicate with each other through those JDG servers. This is standard JDG Cross-DC setup. See the picture below for what the demo looked like: &lt;p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://3.bp.blogspot.com/-nBIDom2q4zI/WydUZMN_5DI/AAAAAAAANnw/95OlEmQMfQsZ98RBsv8t-twA8GtrGYbhgCLcBGAs/s1600/cross-dc-blog-actual-setup-architecture-rhsso.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="https://3.bp.blogspot.com/-nBIDom2q4zI/WydUZMN_5DI/AAAAAAAANnw/95OlEmQMfQsZ98RBsv8t-twA8GtrGYbhgCLcBGAs/s1600/cross-dc-blog-actual-setup-architecture-rhsso.png" data-original-width="727" data-original-height="789" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;The JDG servers were not used during the demo just for the purpose of the RH-SSO, but also for the purpose of other parts of the demo. The details are described in the other blog by Sebastian Laskawiec. The JDG servers were setup with ASYNC backups, which was more effective and was completely fine for the purpose of the demo due the fact that mobile application was using keycloak.js adapter. See &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.2/html/server_installation_and_configuration_guide/operating-mode#backups"&gt;RH-SSO docs&lt;/a&gt; for more details. &lt;h2&gt;Red Hat SSO customizations&lt;/h2&gt; &lt;p&gt;The RH-SSO was using standard &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_middleware_for_openshift/3/html/red_hat_single_sign-on_for_openshift/"&gt;RH-SSO openshift image&lt;/a&gt; . For Cross-DC setup, we needed to do configuration changes as described in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.2/html/server_installation_and_configuration_guide/operating-mode#crossdc-mode"&gt;RHSSO documentation&lt;/a&gt; . Also few other customizations were done. &lt;h3&gt;JDG User Storage&lt;/h3&gt; &lt;p&gt;RH-SSO Cross-DC setup currently requires both replicated RDBMS and replicated JDG server. When preparing to demo, we figured that using the clustered RDBMS in OpenShift replicated across all 3 clouds, is not very straightforward thing to setup. &lt;p&gt;Fortunately RH-SSO is highly customizable platform and among other things, it provides supported &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.2/html/server_administration_guide/user-storage-federation"&gt;User Storage SPI&lt;/a&gt; , which allows customers to plug their own storage for RH-SSO users. So instead of setup of replicated RDBMS, we created custom JDG User Storage. So users of the example realm were saved inside JDG instead of the RDBMS Database. &lt;p&gt;Lessons learned is, that we want to make the Keycloak/RH-SSO Cross-DC setup simpler for administrators. Hence we're considering removing the need for replicated RDBMS entirely and instead store all realms and users metadata within JDG. So just replicated JDG would be a requirement for Cross-DC setup. &lt;h3&gt;Other customizations&lt;/h3&gt; &lt;p&gt;For the purpose of the demo, we did custom login theme. We also did Email-Only authenticator, which allows to register user just by providing their email address. This is obviously not very secure, but it's pretty neat for the example purpose. Keynote users were also able to login with &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.2/html/server_administration_guide/identity_broker#google"&gt;Google Identity Provider&lt;/a&gt; or &lt;a href="https://developers.redhat.com/"&gt;Red Hat Developers OpenID Connect Identity Provider&lt;/a&gt;, which was useful for users, who already had an account in those services. &lt;p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://2.bp.blogspot.com/-Ge7V9OICypw/WydV-ODGNmI/AAAAAAAANn8/H7EmDQ4SfQshbawCjIpkKISnM6ZwW2QrACLcBGAs/s1600/login-screen.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="https://2.bp.blogspot.com/-Ge7V9OICypw/WydV-ODGNmI/AAAAAAAANn8/H7EmDQ4SfQshbawCjIpkKISnM6ZwW2QrACLcBGAs/s1600/login-screen.png" data-original-width="1600" data-original-height="857" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;If you want to try all these things in action, you can try to checkout our &lt;a href="https://github.com/rhdemo/rh-sso"&gt;Demo Project on Github&lt;/a&gt; and deploy it to your own openshift cluster! If you have 3 clouds, even better! You can try the full setup including JDG to try exactly the setup we used during keynote demo.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/keYdteRKPD4" height="1" width="1" alt=""/&gt;</content><summary>Red Hat Summit is one of the most important events during the year. Many geeks, Red Hat employees and customers have great opportunity to meet, learn new things and attend lots of interesting presentations and trainings. During the summit this year, there were few breakout sessions, which were solely about Keycloak and Red Hat SSO. You can take a look at this blogpost for more details. One of the ...</summary><dc:creator>Marek Posolda</dc:creator><dc:date>2018-06-18T06:59:00Z</dc:date><feedburner:origLink>http://blog.keycloak.org/2018/06/red-hat-single-sign-on-in-keynote-demo.html</feedburner:origLink></entry><entry><title>New: Asynchronous container filters</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/4q7RjLd5rj4/new-asynchronous-container-filters" /><category term="feed_group_name_resteasy" scheme="searchisko:content:tags" /><category term="feed_name_resteasy" scheme="searchisko:content:tags" /><author><name>Stephane Epardaud</name></author><id>searchisko:content:id:jbossorg_blog-new_asynchronous_container_filters</id><updated>2018-06-18T04:03:55Z</updated><published>2018-06-18T04:03:55Z</published><content type="html">&lt;!-- [DocumentBodyStart:4708eeb8-6e56-4321-a441-65e791e31ff6] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;JAX-RS 2.0 shipped with support for filtering &lt;a class="jive-link-external-small" href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/ContainerRequestFilter.html" rel="nofollow"&gt;requests&lt;/a&gt; and &lt;a class="jive-link-external-small" href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/ContainerResponseFilter.html" rel="nofollow"&gt;responses&lt;/a&gt;, which enabled a lot of great use-cases for delegating duplicated code away from resources and into filters that would do the same processing for every resource method.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Request filters work by overriding the &lt;a class="jive-link-external-small" href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/ContainerRequestFilter.html#filter-javax.ws.rs.container.ContainerRequestContext-" rel="nofollow"&gt;ContainerRequestFilter.filter&lt;/a&gt; method and observe or modify the given &lt;a class="jive-link-external-small" href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/ContainerRequestContext.html" rel="nofollow"&gt;context&lt;/a&gt; object, or &lt;a class="jive-link-external-small" href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/ContainerRequestContext.html#abortWith-javax.ws.rs.core.Response-" rel="nofollow"&gt;abort the filter chain&lt;/a&gt; with a response if the filter already has a response and the other filters and resource method are not required. Simply returning from the filter method will cause the next filter to be called, or when we have run all the filters, it will invoke the resource method.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Response filters are very similar, but &lt;a class="jive-link-external-small" href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/ContainerResponseFilter.html" rel="nofollow"&gt;execute&lt;/a&gt; after the resource method has been executed and &lt;a class="jive-link-external-small" href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/ContainerResponseContext.html" rel="nofollow"&gt;produced an entity, status code, headers&lt;/a&gt;, which the filter can then modify if required, or simply return to let the next filters run, or the response be sent to the client.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;This is all great, but how does it work in an asynchronous ecosystem ? It doesn't, really, because even though JAX-RS supports &lt;a class="jive-link-external-small" href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/Suspended.html" rel="nofollow"&gt;suspending the request&lt;/a&gt;, it only supports it within the resource method: filters are too early (for request filters), or too late (for response filters).&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;In RESTEasy 3.5 and 4.0.0, we introduced the ability to &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.0.Final/userguide/html/Interceptors.html#d4e1819" rel="nofollow"&gt;suspend the request in filters&lt;/a&gt;. To do that, write your request or response filter as usual, but then cast your context object down to &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.0.Final/javadocs/org/jboss/resteasy/core/interception/jaxrs/SuspendableContainerRequestContext.html" rel="nofollow"&gt;SuspendableContainerRequestContext&lt;/a&gt; or &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.0.Final/javadocs/org/jboss/resteasy/core/interception/jaxrs/SuspendableContainerResponseContext.html" rel="nofollow"&gt;SuspendableContainerResponseContext&lt;/a&gt; (for response filters), and you can then:&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;- suspend the request with &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.0.Final/javadocs/org/jboss/resteasy/core/interception/jaxrs/SuspendableContainerRequestContext.html#suspend--" rel="nofollow"&gt;SuspendableContainerRequestContext.suspend()&lt;/a&gt;&lt;/p&gt;&lt;p&gt;- resume it normally with &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.0.Final/javadocs/org/jboss/resteasy/core/interception/jaxrs/SuspendableContainerRequestContext.html#resume--" rel="nofollow"&gt;SuspendableContainerRequestContext.resume()&lt;/a&gt;, to proceed to the next filter or resource method&lt;/p&gt;&lt;p&gt;- resume it with a response with the standard &lt;a class="jive-link-external-small" href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/ContainerRequestContext.html#abortWith-javax.ws.rs.core.Response-" rel="nofollow"&gt;ContainerRequestContext.abortWith()&lt;/a&gt;, to directly send that response to the client&lt;/p&gt;&lt;p&gt;- resume it with an exception with &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.0.Final/javadocs/org/jboss/resteasy/core/interception/jaxrs/SuspendableContainerRequestContext.html#resume-java.lang.Throwable-" rel="nofollow"&gt;SuspendableContainerRequestContext.resume(Throwable)&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Similarly, for response filters, you can:&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;- suspend the request with &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.0.Final/javadocs/org/jboss/resteasy/core/interception/jaxrs/SuspendableContainerResponseContext.html#suspend--" rel="nofollow"&gt;SuspendableContainerResponseContext.suspend()&lt;/a&gt;&lt;/p&gt;&lt;p&gt;- resume it normally with &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.0.Final/javadocs/org/jboss/resteasy/core/interception/jaxrs/SuspendableContainerResponseContext.html#resume--" rel="nofollow"&gt;SuspendableContainerResponseContext.resume()&lt;/a&gt;, to proceed to the next filter or return the response to the client&lt;/p&gt;&lt;p&gt;- resume it with an exception with &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.0.Final/javadocs/org/jboss/resteasy/core/interception/jaxrs/SuspendableContainerResponseContext.html#resume-java.lang.Throwable-" rel="nofollow"&gt;SuspendableContainerResponseContext.resume(Throwable)&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Of course, the &lt;span style="font-family: 'andale mono', times;"&gt;resume()&lt;/span&gt; methods only work after you've called &lt;span style="font-family: 'andale mono', times;"&gt;suspend()&lt;/span&gt;, but otherwise you can call &lt;span style="font-family: 'andale mono', times;"&gt;resume()&lt;/span&gt; right after &lt;span style="font-family: 'andale mono', times;"&gt;suspend()&lt;/span&gt;, before returning from the filter, in which case the request will not even be made asynchronous, or you can call &lt;span style="font-family: 'andale mono', times;"&gt;resume()&lt;/span&gt; later after you return from the method, or even from another thread entirely, in which case the request will become asynchronous.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;The fact that filters may turn requests asynchronous has no impact at all on the rest of your code: non-asynchronous and asynchronous resource methods continue to work exactly as normal, regardless of the asynchronous status of the request, so you don't need to modify your code to accommodate for asynchronous filters.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;h3&gt;Asynchronous rate-limiter example with Redis&lt;/h3&gt;&lt;h3&gt;&lt;/h3&gt;&lt;p&gt;Asynchronous filters are useful for plugging in anything that requires asynchrony, such as reactive security frameworks, async response processing or async caching. We will illustrate how to use asynchronous filters with a rate-limiter example.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;For that, we will use &lt;a class="jive-link-external-small" href="https://github.com/mokies/ratelimitj/tree/master/ratelimitj-redis" rel="nofollow"&gt;RateLimitJ for Redis&lt;/a&gt;, which uses Redis to store rate-limiting information for your API. This is very useful for sharing rate-limit between your API server cluster, because you can store that info in a Redis cluster, and you don't have to worry about blocking clients while you're waiting for Redis to give you the info: you just become asynchronous until you have an answer from Redis.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;We will first import the right Maven dependency for RateLimitJ:&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;!--[CodeBlockStart:16745537-bb02-4f39-a550-831dbd42ec31][excluded]--&gt;&lt;pre class="xml" name="code"&gt;&amp;lt;dependency&amp;gt; &amp;#160; &amp;lt;groupId&amp;gt;es.moki.ratelimitj&amp;lt;/groupId&amp;gt; &amp;#160; &amp;lt;artifactId&amp;gt;ratelimitj-redis&amp;lt;/artifactId&amp;gt; &amp;#160; &amp;lt;version&amp;gt;0.4.2&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &lt;/pre&gt;&lt;!--[CodeBlockEnd:16745537-bb02-4f39-a550-831dbd42ec31]--&gt;&lt;div style="display:none;"&gt;&lt;/div&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;And let's not forget to &lt;a class="jive-link-external-small" href="https://redis.io/download" rel="nofollow"&gt;install and run a local Redis cluster&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;We will start by declaring a &lt;span style="font-family: 'andale mono', times;"&gt;@RateLimit&lt;/span&gt; annotation that we can use on our resource methods or classes to indicate we want rate limiting:&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;!--[CodeBlockStart:8f1f7376-90ee-4b7b-9e55-e1ea73653c26][excluded]--&gt;&lt;pre class="java" name="code"&gt;@Documented @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.TYPE, ElementType.METHOD}) public @interface RateLimit { /** * Number of {@link #unit()} that defines our sliding window. */ int duration(); /** * Unit used for the sliding window {@link #duration()}. */ TimeUnit unit(); /** * Maximum number of requests to allow during our sliding window. */ int maxRequest(); }&lt;/pre&gt;&lt;!--[CodeBlockEnd:8f1f7376-90ee-4b7b-9e55-e1ea73653c26]--&gt;&lt;div style="display:none;"&gt;&lt;/div&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;And we have to declare a &lt;span style="font-family: 'andale mono', times;"&gt;DynamicFeature&lt;/span&gt; that enables the filter on annotated methods and classes:&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;!--[CodeBlockStart:5fe56e1e-a859-4969-a26a-7ec0ad514570][excluded]--&gt;&lt;pre class="java" name="code"&gt;@Provider public class RateLimitFeature implements DynamicFeature { &amp;#160; private StatefulRedisConnection&amp;lt;string,string&amp;gt; connection; &amp;#160; public RateLimitFeature(){ &amp;#160;&amp;#160;&amp;#160; // connect to the local Redis &amp;#160;&amp;#160;&amp;#160; connection = RedisClient.create("redis://localhost").connect(); &amp;#160; } &amp;#160; public void configure(ResourceInfo resourceInfo, FeatureContext context) { &amp;#160;&amp;#160;&amp;#160; // See if we're rate-limiting &amp;#160;&amp;#160;&amp;#160; RateLimit limit = resourceInfo.getResourceMethod().getAnnotation(RateLimit.class); &amp;#160;&amp;#160;&amp;#160; if(limit == null) &amp;#160;&amp;#160;&amp;#160; limit = resourceInfo.getResourceClass().getAnnotation(RateLimit.class); &amp;#160;&amp;#160;&amp;#160; if(limit != null) { &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; // add the rate-limiting filter &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; Set rules = new HashSet&amp;lt;&amp;gt;(); &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; rules.add(RequestLimitRule.of(limit.duration(), limit.unit(), limit.maxRequest())); &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; context.register(new RateLimitFilter(new RedisSlidingWindowRequestRateLimiter(connection, rules))); &amp;#160;&amp;#160;&amp;#160; } &amp;#160; } }&lt;/pre&gt;&lt;!--[CodeBlockEnd:5fe56e1e-a859-4969-a26a-7ec0ad514570]--&gt;&lt;div style="display:none;"&gt;&lt;/div&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;And this is how we implement our asynchronous filter:&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;!--[CodeBlockStart:d1b0ec64-85b3-4ed3-aede-0ee6727ece97][excluded]--&gt;&lt;pre class="java" name="code"&gt;public class RateLimitFilter implements ContainerRequestFilter { &amp;#160; private RedisSlidingWindowRequestRateLimiter requestRateLimiter; &amp;#160; public RateLimitFilter(RedisSlidingWindowRequestRateLimiter requestRateLimiter) { &amp;#160;&amp;#160;&amp;#160; this.requestRateLimiter = requestRateLimiter; &amp;#160; } &amp;#160; public void filter(ContainerRequestContext requestContext) throws IOException { &amp;#160;&amp;#160;&amp;#160; // Get access to the remote address &amp;#160;&amp;#160;&amp;#160; HttpServletRequest servletRequestContext = ResteasyProviderFactory.getContextData(HttpServletRequest.class); &amp;#160;&amp;#160;&amp;#160; // Suspend the request &amp;#160;&amp;#160;&amp;#160; SuspendableContainerRequestContext suspendableRequestContext = (SuspendableContainerRequestContext) requestContext; &amp;#160;&amp;#160;&amp;#160; suspendableRequestContext.suspend(); &amp;#160;&amp;#160;&amp;#160; // Query and increment by remote IP &amp;#160;&amp;#160;&amp;#160; requestRateLimiter.overLimitAsync("ip:"+servletRequestContext.getRemoteAddr()) &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; .whenComplete((overlimit, error) -&amp;gt; { &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; // Error case &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; if(error != null) &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; suspendableRequestContext.resume(error); &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; // Over limit &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; else if(overlimit) &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; suspendableRequestContext.abortWith(Response.status(429).build()); &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; // Good to go! &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; else &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; suspendableRequestContext.resume(); &amp;#160;&amp;#160;&amp;#160;&amp;#160;&amp;#160; }); &amp;#160; } }&lt;/pre&gt;&lt;!--[CodeBlockEnd:d1b0ec64-85b3-4ed3-aede-0ee6727ece97]--&gt;&lt;div style="display:none;"&gt;&lt;/div&gt;&lt;p&gt;Now all we have left to do is to implement a resource with rate-limiting:&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;!--[CodeBlockStart:4d3b4c7c-0590-48cc-9645-9a51f0a00ce5][excluded]--&gt;&lt;pre class="java" name="code"&gt;@Path("/") public class Resource { &amp;#160; @Path("free") &amp;#160; @GET &amp;#160; public String free() { &amp;#160;&amp;#160;&amp;#160; return "Hello Free World"; &amp;#160; } &amp;#160; @RateLimit(duration = 10, unit = TimeUnit.SECONDS, maxRequest = 2) &amp;#160; @Path("limited") &amp;#160; @GET &amp;#160; public String limited() { &amp;#160;&amp;#160;&amp;#160; return "Hello Limited World"; &amp;#160; } }&lt;/pre&gt;&lt;!--[CodeBlockEnd:4d3b4c7c-0590-48cc-9645-9a51f0a00ce5]--&gt;&lt;div style="display:none;"&gt;&lt;/div&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;If you go to &lt;span style="font-family: 'andale mono', times;"&gt;/free&lt;/span&gt; you will get an unlimited number of requests, while if you go to &lt;span style="font-family: 'andale mono', times;"&gt;/limited&lt;/span&gt; you will get two requests allowed every 10 seconds. The rest of the time you will get an HTTP response of &lt;a class="jive-link-external-small" href="https://tools.ietf.org/html/rfc6585#section-4" rel="nofollow"&gt;Too Many Requests (429)&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;If you have the need for &lt;a class="jive-link-external-small" href="http://docs.jboss.org/resteasy/docs/3.5.1.Final/userguide/html/Interceptors.html#d4e1831" rel="nofollow"&gt;asynchronous request or response filters&lt;/a&gt;, don't hesitate to give RESTEasy &lt;span style="font-family: 'andale mono', times;"&gt;3.5.1.Final&lt;/span&gt; or &lt;span style="font-family: 'andale mono', times;"&gt;4.0.0.Beta2&lt;/span&gt; a try.&lt;/p&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:4708eeb8-6e56-4321-a441-65e791e31ff6] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/4q7RjLd5rj4" height="1" width="1" alt=""/&gt;</content><summary>JAX-RS 2.0 shipped with support for filtering requests and responses, which enabled a lot of great use-cases for delegating duplicated code away from resources and into filters that would do the same processing for every resource method.   Request filters work by overriding the ContainerRequestFilter.filter method and observe or modify the given context object, or abort the filter chain with a res...</summary><dc:creator>Stephane Epardaud</dc:creator><dc:date>2018-06-18T04:03:55Z</dc:date><feedburner:origLink>https://developer.jboss.org/community/resteasy/blog/2018/06/18/new-asynchronous-container-filters</feedburner:origLink></entry><entry><title>Jakarta EE and OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/gPNg-VgmQO4/jakarta-ee-and-openshift" /><category term="devnation" scheme="searchisko:content:tags" /><category term="feed_group_name_management" scheme="searchisko:content:tags" /><category term="feed_name_marklittle" scheme="searchisko:content:tags" /><category term="jakarta ee" scheme="searchisko:content:tags" /><category term="microprofile" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><author><name>Mark Little</name></author><id>searchisko:content:id:jbossorg_blog-jakarta_ee_and_openshift</id><updated>2018-06-17T13:09:21Z</updated><published>2018-06-17T13:09:00Z</published><content type="html">&lt;!-- [DocumentBodyStart:ff43464a-cfdb-4a85-bd0e-e6a6de73bdda] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;At &lt;a class="jive-link-external-small" href="https://www.redhat.com/en/summit/2018" rel="nofollow"&gt;Summit&lt;/a&gt; this year I was able to meet with a number of our key customers and partners. I was also there to give a presentation with Dimitris on &lt;a class="jive-link-external-small" href="https://agenda.summit.redhat.com/SessionDetail.aspx?id=153822" rel="nofollow"&gt;the future of enterprise Java&lt;/a&gt;, which was based a bit on an earlier &lt;a class="jive-link-external-small" href="https://developers.redhat.com/blog/2018/05/02/devnation-jakarta-ee-the-future-of-java-ee/" rel="nofollow"&gt;DevNation Live&lt;/a&gt; session I gave. Now at the time I submitted the session I had no idea which customers would be there in San Francisco but it's always interesting to see how these things turn out because they came together very fortuitously. During the presentation we spoke about Jakarra EE and how it should bring together the Java communities to drive the evolution of enterprise Java towards cloud native. There was a lot of interest in the audience but it was afterwards that I probably had my most beneficial engagements with customers and analysts on the topic. The common thread with all of these meetings was that they had so-called monoliths and worried that they were being left behind by their competitors who were moving to microservices. Or so they believed.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;If you are a student of history you may know about the early cold war where the US thought the USSR had huge stockpiles of nuclear weapons and used that belief to drive their own weapons plans, when in reality the USSR had very few and were just as ill informed as the Americans. I see the same with microservices: everyone believes they understand them, everyone worries their competitors are using them to get ahead and everyone wants them, when in reality they are fine with what they have or they don't need to move quite so quickly towards a new microservices architecture. But how does this play with the future of enterprise Java and specifically &lt;a class="jive-link-blog-small" data-containerId="1427" data-containerType="37" data-objectId="6170" data-objectType="38" href="https://developer.jboss.org/blogs/mark.little/2018/02/28/jakarta-ee-onward"&gt;Jakarta EE&lt;/a&gt;? Well the aim with this industry wide effort is to allow developers to evolve their implementations (monoliths) towards more cloud native, agile implementations using microservices just as we have been doing with &lt;a class="jive-link-external-small" href="http://wildfly-swarm.io/" rel="nofollow"&gt;Eclipse MicroProfile&lt;/a&gt; based on Java EE. At their own pace. Using the skills their teams have built up over nearly two decades. Not throwing away experience but building upon it! Making data driven decisions based upon the needs to customers and their own developers. And if we can do this through the power of open source then let's add another benefit: collaboratively.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;This was a key thing that the audience came away with and which our customers latched onto: don't throw away all you have learned and which works but build on it, evolve it. Evolve the people and processes as much as the software. That can give you the advantage over your competition if they are having to adopt entirely new stacks or frameworks or languages. Your advantage isn't in the software but in the people who developed it and know how your business works and why! Retaining them and their skills are key. They are the assets which cannot be obtained by contractors or consultants.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Another thing which may not be immediately apparent is that Jakarta EE is a prime example of what &lt;a class="jive-link-external-small" href="http://www.tom-ridge.com/2015-09-11_richard_hamming_1968_turing_award_lecture_quote.html" rel="nofollow"&gt;Hamming&lt;/a&gt; was taking about in his Turing Award speech by building on the shoulders of giants rather than reinventing the wheel: so much real world experience has defined the landscape of enterprise Java, of which Java EE plays a key part, and we don't want to lose that experience or the mature implementations based upon it. We want new systems to be as stable and reliable as old and to accomplish that we need many of the skilled developers and a lot of the software! Jakarta EE offers the best path forward for both!&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Now while no Jakarta EE implementations yet exist we do have some work that is relevant. There's WildFly Swarm for a start and then of course there's EAP. Both available through &lt;a class="jive-link-external-small" href="https://developers.redhat.com/products/rhoar/overview/" rel="nofollow"&gt;RHOAR&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:ff43464a-cfdb-4a85-bd0e-e6a6de73bdda] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/gPNg-VgmQO4" height="1" width="1" alt=""/&gt;</content><summary>At Summit this year I was able to meet with a number of our key customers and partners. I was also there to give a presentation with Dimitris on the future of enterprise Java, which was based a bit on an earlier DevNation Live session I gave. Now at the time I submitted the session I had no idea which customers would be there in San Francisco but it's always interesting to see how these things tur...</summary><dc:creator>Mark Little</dc:creator><dc:date>2018-06-17T13:09:00Z</dc:date><feedburner:origLink>https://developer.jboss.org/blogs/mark.little/2018/06/17/jakarta-ee-and-openshift</feedburner:origLink></entry><entry><title>jBPM 7.8 native execution of BPMN2, DMN 1.1 and CMMN 1.1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/7zMguQyzoSs/jbpm-78-native-execution-of-bpmn2-dmn.html" /><category term="case_cmmn" scheme="searchisko:content:tags" /><category term="case_mgmt" scheme="searchisko:content:tags" /><category term="feed_group_name_jbossjbpmcommunity" scheme="searchisko:content:tags" /><category term="feed_name_swiderskimaciej" scheme="searchisko:content:tags" /><category term="jbpm_7.8" scheme="searchisko:content:tags" /><category term="jbpm_bpmn2" scheme="searchisko:content:tags" /><category term="jbpm_cases" scheme="searchisko:content:tags" /><category term="jbpm_cmmn" scheme="searchisko:content:tags" /><category term="jbpm_dmn" scheme="searchisko:content:tags" /><author><name>Maciej Swiderski</name></author><id>searchisko:content:id:jbossorg_blog-jbpm_7_8_native_execution_of_bpmn2_dmn_1_1_and_cmmn_1_1</id><updated>2018-06-15T12:58:28Z</updated><published>2018-06-15T12:58:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;with upcoming 7.8 release of jBPM there is quite nice thing to announce - native execution of:&lt;br /&gt;&lt;br /&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;BPMN2 - was there already for many years&lt;/li&gt;&lt;li&gt;DMN 1.1 - from the early days of version 7&lt;/li&gt;&lt;li&gt;CMMN 1.1 - comes with version 7.8&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;br /&gt;The biggest thing coming with 7.8 is actually CMMN execution. It is mainly added for completeness of the execution so people who would like to model case with CMMN can actually execute that directly on jBPM (via KIE Server or embedded).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Although jBPM supports now CMMN, it is still recommended to use BPMN2 and case management features of jBPM for advanced cases to benefit from features that both specification brings rather to be limited to particular approach. Nevertheless CMMN can be a good visualisation for less complex cases where data and loosely coupled activities can build a good business view.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;Disclaimer: jBPM currently does not provide nor plans to provide any modelling capabilities for CMMN.&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;With that said let's take a quick look at what is supported from the CMMN specification as obviously it's not covering 100% of the spec.&lt;/div&gt;&lt;br /&gt;&lt;div&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;tasks (human task, process task, decision task, case task)&lt;/li&gt;&lt;li&gt;discretionary tasks (same as above)&lt;/li&gt;&lt;li&gt;stages&lt;/li&gt;&lt;li&gt;milestones&lt;/li&gt;&lt;li&gt;case file items&lt;/li&gt;&lt;li&gt;sentries (both entry and exit)&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;&lt;div&gt;Not all attributes of tasks are supported - required, repeat and manual activation are currently not supported. Although most of the behaviour can still be achieved using different constructs.&lt;/div&gt;&lt;div&gt;Sentries for individual tasks are limited to entry criteria while entry and exit are supported for stages and milestones.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Decision task by default maps to DMN decision although ruleflow group based is also possible with simplified syntax - decisionRef should be set to ruleflow-group attribute.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Event listeners are not supported as they do not bring much value for execution and instead CaseEventListener support in jBPM should be used as substitute.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Let's have a quick look at how the sample Order IT case would look like designed in CMMN&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://4.bp.blogspot.com/-YDMiMFfpR1w/WyOvBRxMgfI/AAAAAAAABdg/d8DiMkhLlcUzJa9_6hVNr4SN9ZbYYSDjQCLcBGAs/s1600/Screen%2BShot%2B2018-06-15%2Bat%2B14.07.45.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="727" data-original-width="1421" height="326" src="https://4.bp.blogspot.com/-YDMiMFfpR1w/WyOvBRxMgfI/AAAAAAAABdg/d8DiMkhLlcUzJa9_6hVNr4SN9ZbYYSDjQCLcBGAs/s640/Screen%2BShot%2B2018-06-15%2Bat%2B14.07.45.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;some might say it's less or more readable and frankly speaking it's just a matter of preferences.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here is a screencast showing this CMMN model being executed&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;iframe allow="autoplay; encrypted-media" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/wYtsHUwCgWc" width="560"&gt;&lt;/iframe&gt; &lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Next I'd like to show the true power of jBPM - execution of all three types of models:&lt;/div&gt;&lt;div&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;CMMN for top level case definition&lt;/li&gt;&lt;li&gt;DMN for decision service&lt;/li&gt;&lt;li&gt;BPMN2 for process execution&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;you can add all of them into kjar (via import asset in workbench) build, deploy from workbench directly to KIE Server and execute. So here are our assets&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;A case definition that has:&lt;/div&gt;&lt;div&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;decision task that invokes DMN decision that calculates vacation days (Total Vacation Days)&lt;/li&gt;&lt;li&gt;two human tasks that are triggered based on the data (entry criterion)&lt;/li&gt;&lt;li&gt;process task that invokes BPMN2 process if the entry condition is met&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://2.bp.blogspot.com/-GxcN8ZrhafQ/WyOwDdaXAOI/AAAAAAAABdo/rWJfD90-vWgRV1eqdk3AYVVn27DRzX9lQCLcBGAs/s1600/Screen%2BShot%2B2018-06-15%2Bat%2B14.08.12.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="722" data-original-width="881" height="522" src="https://2.bp.blogspot.com/-GxcN8ZrhafQ/WyOwDdaXAOI/AAAAAAAABdo/rWJfD90-vWgRV1eqdk3AYVVn27DRzX9lQCLcBGAs/s640/Screen%2BShot%2B2018-06-15%2Bat%2B14.08.12.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here is our DMN model&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://4.bp.blogspot.com/-Qz3ivqECBgQ/WyOwwBUXNwI/AAAAAAAABdw/rxA3p8JUbMM2ghWVz1ySBA_gnqOZkhCfwCLcBGAs/s1600/Screen%2BShot%2B2018-06-15%2Bat%2B14.08.33.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="607" data-original-width="1013" height="382" src="https://4.bp.blogspot.com/-Qz3ivqECBgQ/WyOwwBUXNwI/AAAAAAAABdw/rxA3p8JUbMM2ghWVz1ySBA_gnqOZkhCfwCLcBGAs/s640/Screen%2BShot%2B2018-06-15%2Bat%2B14.08.33.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;and last but not least is the BPMN2 process (actually the most simple one but still a valid process)&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-v_3ZlVoIf9k/WyOw1bLDmcI/AAAAAAAABd0/nKyRaEZWIbgvWs7O76_sanMbfLcjEQgkQCLcBGAs/s1600/Screen%2BShot%2B2018-06-15%2Bat%2B14.09.10.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="272" data-original-width="606" height="286" src="https://1.bp.blogspot.com/-v_3ZlVoIf9k/WyOw1bLDmcI/AAAAAAAABd0/nKyRaEZWIbgvWs7O76_sanMbfLcjEQgkQCLcBGAs/s640/Screen%2BShot%2B2018-06-15%2Bat%2B14.09.10.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Another thing to mention is that, all the models where done with Tristotech Editors to illustrate that they can be simply created with another tool and imported into kjar for execution.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here is another screencast showing this all step by step, from exporting from Tristotech, importing into workbench, building and deploying kjar and lastly execute on KIE Server.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;iframe allow="autoplay; encrypted-media" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/R2D7SFqTbNo" width="560"&gt;&lt;/iframe&gt; &lt;div&gt;&lt;br /&gt;That's all to share for now, 7.8 is just around the corner so keep your eyes open and visit &lt;a href="http://jbpm.org/"&gt;jbpm.org&lt;/a&gt; to learn more.&lt;br /&gt;&lt;br /&gt;And at the end here are the links to the projects (kjars) in github&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;&lt;a href="https://github.com/mswiderski/cmmn-itorders"&gt;CMMN Order IT case sample&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/mswiderski/cmmn-dmn-bpmn"&gt;CMMN, DMN, BPMN sample&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Enjoy!&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/7zMguQyzoSs" height="1" width="1" alt=""/&gt;</content><summary>with upcoming 7.8 release of jBPM there is quite nice thing to announce - native execution of: BPMN2 - was there already for many years DMN 1.1 - from the early days of version 7 CMMN 1.1 - comes with version 7.8 The biggest thing coming with 7.8 is actually CMMN execution. It is mainly added for completeness of the execution so people who would like to model case with CMMN can actually execute th...</summary><dc:creator>Maciej Swiderski</dc:creator><dc:date>2018-06-15T12:58:00Z</dc:date><feedburner:origLink>http://mswiderski.blogspot.com/2018/06/jbpm-78-native-execution-of-bpmn2-dmn.html</feedburner:origLink></entry><entry><title>Announcing .NET Core 2.1 for Red Hat Platforms</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aUpfi5Mos60/" /><category term=".net" scheme="searchisko:content:tags" /><category term=".NET Core" scheme="searchisko:content:tags" /><category term="C#" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="Developer Tools" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><author><name>Bob Davis</name></author><id>searchisko:content:id:jbossorg_blog-announcing_net_core_2_1_for_red_hat_platforms</id><updated>2018-06-14T16:29:50Z</updated><published>2018-06-14T16:29:50Z</published><content type="html">&lt;p&gt;&lt;span style="font-weight: 400"&gt;We are very pleased to announ&lt;/span&gt;&lt;span style="font-weight: 400"&gt;ce the general availability of .NET Core 2.1 for Red Hat Enterprise Linux and OpenShift platforms!&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400"&gt;.NET Core is the open-source, cross-platform .NET platform for building microservices. .NET Core is designed to provide the best performance at scale for applications that use microservices and containers. Libraries can be shared with other .NET platforms, such as .NET Framework (Windows) and Xamarin (mobile applications). With .NET Core you have the flexibility of building and deploying applications on Red Hat Enterprise Linux or in containers. Your container-based applications and microservices can easily be deployed to your choice of public or private clouds using Red Hat OpenShift. All of the features of OpenShift and Kubernetes for cloud deployments are available to you.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400"&gt;.NET Core 2.1 continues to broaden its support and tools for microservice development in an open source environment. The latest version of .NET Core includes the following improvements:&lt;/span&gt;&lt;span id="more-498747"&gt;&lt;/span&gt;&lt;/p&gt; &lt;ul&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Improved build performance&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Improved runtime performance&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Improved networking performance&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;The new &lt;code&gt;Span&amp;#60;T&amp;#62;&lt;/code&gt; based APIs for reducing allocations&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Extended Cryptography APIs&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;API Support for Brotli compression&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;A new way of deploying tools as NuGet Packages&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span style="font-weight: 400"&gt;This release further reduces platform differences between Windows and Linux.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400"&gt;As usual, .NET Core 2.1 is available via &lt;/span&gt;&lt;a href="https://developers.redhat.com/products/dotnet/hello-world/"&gt;&lt;span style="font-weight: 400"&gt;traditional “yum” install&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400"&gt; (rh-dotnet21) or in containers through our &lt;/span&gt;&lt;a href="https://access.redhat.com/containers/#/search/dotnet"&gt;&lt;span style="font-weight: 400"&gt;container catalog&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400"&gt;.&lt;/span&gt;&lt;/p&gt; &lt;h3&gt;&lt;span style="font-weight: 400"&gt;Release and support information&lt;/span&gt;&lt;/h3&gt; &lt;p&gt;&lt;span style="font-weight: 400"&gt;Developers may use .NET Core 2.1 to develop and deploy applications on:&lt;/span&gt;&lt;/p&gt; &lt;ul&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Red Hat Enterprise Linux&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Red Hat Enterprise Linux Atomic Host&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Red Hat OpenShift Container Platform  &lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Red Hat OpenShift Online&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Red Hat OpenShift Dedicated&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Red Hat OpenStack Platform&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span style="font-weight: 400"&gt;Following a small number of significant releases in the next few months, .NET Core 2.1 is expected to switch to long-term support (LTS) release as described in the &lt;/span&gt;&lt;a href="https://access.redhat.com/support/policy/updates/net-core"&gt;&lt;span style="font-weight: 400"&gt;Red Hat’s lifecycle documentation&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400"&gt;. This means that critical updates addressing security and reliability will be offered for 3 years. &lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400"&gt;For more information, please visit the following:&lt;/span&gt;&lt;/p&gt; &lt;ol&gt; &lt;li style="font-weight: 400"&gt;&lt;a href="https://access.redhat.com/documentation/en-us/net_core/2.1/html-single/getting_started_guide/index"&gt;&lt;span style="font-weight: 400"&gt;Get Started with .NET Core 2.1!&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Visit &lt;/span&gt;&lt;a href="http://redhatloves.net/"&gt;&lt;span style="font-weight: 400"&gt;RedHatLoves.NET&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;The Red Hat Developer Program &lt;/span&gt;&lt;a href="https://developers.redhat.com/products/dotnet/overview/"&gt;&lt;span style="font-weight: 400"&gt;technology page on .NET Core&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400"&gt;.&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;span style="font-weight: 400"&gt;Red Hat Developer &lt;/span&gt;&lt;a href="https://developers.redhat.com/blog/category/programming/dot-net/"&gt;&lt;span style="font-weight: 400"&gt;blogs&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400"&gt; on .NET Core&lt;/span&gt;&lt;/li&gt; &lt;li style="font-weight: 400"&gt;&lt;a href="https://access.redhat.com/documentation/en-us/net_core/2.1/"&gt;&lt;span style="font-weight: 400"&gt;Product Documentation for .NET Core&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;span style="font-weight: 400"&gt;For complete information on the updates and changes made in this release, &lt;/span&gt;&lt;a href="https://github.com/dotnet/core/blob/master/release-notes/2.1/2.1.0.md"&gt;&lt;span style="font-weight: 400"&gt;please visit the project page on GitHub&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400"&gt;. Any important differences between Red Hat’s official source build and other builds that are available will be &lt;/span&gt;&lt;a href="https://access.qa.redhat.com/documentation/en-us/net_core/2.1/"&gt;&lt;span style="font-weight: 400"&gt;detailed in our release notes&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400"&gt;. &lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fannouncing-net-core-2-1-for-red-hat-platforms%2F&amp;#38;linkname=Announcing%20.NET%20Core%202.1%20for%20Red%20Hat%20Platforms" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fannouncing-net-core-2-1-for-red-hat-platforms%2F&amp;#38;linkname=Announcing%20.NET%20Core%202.1%20for%20Red%20Hat%20Platforms" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fannouncing-net-core-2-1-for-red-hat-platforms%2F&amp;#38;linkname=Announcing%20.NET%20Core%202.1%20for%20Red%20Hat%20Platforms" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fannouncing-net-core-2-1-for-red-hat-platforms%2F&amp;#38;linkname=Announcing%20.NET%20Core%202.1%20for%20Red%20Hat%20Platforms" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fannouncing-net-core-2-1-for-red-hat-platforms%2F&amp;#38;linkname=Announcing%20.NET%20Core%202.1%20for%20Red%20Hat%20Platforms" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fannouncing-net-core-2-1-for-red-hat-platforms%2F&amp;#38;linkname=Announcing%20.NET%20Core%202.1%20for%20Red%20Hat%20Platforms" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fannouncing-net-core-2-1-for-red-hat-platforms%2F&amp;#38;linkname=Announcing%20.NET%20Core%202.1%20for%20Red%20Hat%20Platforms" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fannouncing-net-core-2-1-for-red-hat-platforms%2F&amp;#38;linkname=Announcing%20.NET%20Core%202.1%20for%20Red%20Hat%20Platforms" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fannouncing-net-core-2-1-for-red-hat-platforms%2F&amp;#38;title=Announcing%20.NET%20Core%202.1%20for%20Red%20Hat%20Platforms" data-a2a-url="https://developers.redhat.com/blog/2018/06/14/announcing-net-core-2-1-for-red-hat-platforms/" data-a2a-title="Announcing .NET Core 2.1 for Red Hat Platforms"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/14/announcing-net-core-2-1-for-red-hat-platforms/"&gt;Announcing .NET Core 2.1 for Red Hat Platforms&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aUpfi5Mos60" height="1" width="1" alt=""/&gt;</content><summary>We are very pleased to announce the general availability of .NET Core 2.1 for Red Hat Enterprise Linux and OpenShift platforms! .NET Core is the open-source, cross-platform .NET platform for building microservices. .NET Core is designed to provide the best performance at scale for applications that use microservices and containers. Libraries can be shared with other .NET platforms, such as .NET Fr...</summary><dc:creator>Bob Davis</dc:creator><dc:date>2018-06-14T16:29:50Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/14/announcing-net-core-2-1-for-red-hat-platforms/</feedburner:origLink></entry><entry><title>Debugging Memory Issues with Open vSwitch DPDK</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ZoZS5zLebLI/" /><category term="debugging" scheme="searchisko:content:tags" /><category term="dpdk" scheme="searchisko:content:tags" /><category term="fast datapath" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="hugepages" scheme="searchisko:content:tags" /><category term="Networking" scheme="searchisko:content:tags" /><category term="NFV" scheme="searchisko:content:tags" /><category term="Open vSwitch" scheme="searchisko:content:tags" /><category term="OpenStack" scheme="searchisko:content:tags" /><category term="openvswitch" scheme="searchisko:content:tags" /><category term="ovs-dpdk" scheme="searchisko:content:tags" /><category term="Red Hat Enterprise Linux" scheme="searchisko:content:tags" /><category term="Virtualization" scheme="searchisko:content:tags" /><author><name>Kevin Traynor</name></author><id>searchisko:content:id:jbossorg_blog-debugging_memory_issues_with_open_vswitch_dpdk</id><updated>2018-06-14T11:00:48Z</updated><published>2018-06-14T11:00:48Z</published><content type="html">&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt; &lt;p&gt;This article is about debugging out-of-memory issues with &lt;a href="http://docs.openvswitch.org/en/latest/intro/install/dpdk/"&gt;Open vSwitch with the Data Plane Development Kit&lt;/a&gt; (OvS-DPDK). It explains the situations in which you can run out of memory when using OvS-DPDK and it shows the log entries that are produced in those circumstances. It also shows some other log entries and commands for further debugging.&lt;/p&gt; &lt;p&gt;When you finish reading this article, you will be able to identify that you have an out-of-memory issue and you&amp;#8217;ll know how to fix it. Spoiler: Usually having some more memory on the relevant NUMA node works. It is based on OvS 2.9.&lt;/p&gt; &lt;p&gt;&lt;span id="more-500667"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2 id="background"&gt;Background&lt;/h2&gt; &lt;p&gt;As is normal with DPDK-type applications, it is expected that hugepage memory has been set up and mounted. For further information see &lt;a href="http://docs.openvswitch.org/en/latest/intro/install/dpdk/?highlight=hugepage#setup-hugepages"&gt;set up huge pages&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The next step is to specify the amount of memory pre-allocated for OvS-DPDK. This is done using the Open vSwitch Database (OVSDB). In the case below, 4GB of huge-page memory is pre-allocated on NUMA node 0 and NUMA node 1.&lt;/p&gt; &lt;pre&gt;# ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=4096,4096&lt;/pre&gt; &lt;p&gt;The default is 1GB for NUMA 0 if &lt;code&gt;dpdk-socket-mem&lt;/code&gt; is not specified.&lt;/p&gt; &lt;p&gt;Now, let&amp;#8217;s look at the times when we can run out of memory.&lt;/p&gt; &lt;h2 id="initialization"&gt;Initialization&lt;/h2&gt; &lt;p&gt;You can run out of memory when DPDK is initialized, which happens when &lt;code&gt;ovs-vswitchd&lt;/code&gt; is running and the OVSDB entry &lt;code&gt;dpdk-init&lt;/code&gt; is set to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;A useful log entry to watch for during initialization is this:&lt;/p&gt; &lt;pre&gt;|dpdk|INFO|EAL ARGS: ovs-vswitchd -c 0x1 --socket-mem 4096,4096&lt;/pre&gt; &lt;p&gt;This will confirm that the &lt;code&gt;dpdk-socket-mem&lt;/code&gt; you &lt;em&gt;thought&lt;/em&gt; you were setting was actually set and passed to DPDK (thus avoiding the embarrassment of someone else pointing out that your scripts were wrong).&lt;/p&gt; &lt;p&gt;The most likely way to run out of memory during initialization is that huge page memory was not set up correctly:&lt;/p&gt; &lt;pre&gt;|dpdk|INFO|EAL ARGS: ovs-vswitchd -c 0x1 --socket-mem 4096,4096 |dpdk|INFO|EAL: 32 hugepages of size 1073741824 reserved, but no mounted hugetlbfs found for that size&lt;/pre&gt; &lt;p&gt;Another way is that you are requesting too much memory:&lt;/p&gt; &lt;pre&gt;|dpdk|INFO|EAL ARGS: ovs-vswitchd -c 0x1 --socket-mem 32768,0 |dpdk|ERR|EAL: Not enough memory available on socket 0! Requested: 32768MB, available: 16384MB&lt;/pre&gt; &lt;p&gt;Or you request none at all:&lt;/p&gt; &lt;pre&gt;|dpdk|INFO|EAL ARGS: ovs-vswitchd -c 0x1 --socket-mem 0,0 |dpdk|ERR|EAL: invalid parameters for --socket-mem&lt;/pre&gt; &lt;p&gt;All these issues can be fixed by correctly setting up huge pages and requesting to pre-allocate an appropriate amount.&lt;/p&gt; &lt;h2 id="adding-a-port-changing-mtu"&gt;Adding a Port or Changing the MTU&lt;/h2&gt; &lt;p&gt;These situations are grouped together because they can both result in a new pool of buffers being requested for a port. Where possible, these pools of buffers will be shared and reused, but that is not always possible due to differing port NUMA nodes or MTUs.&lt;/p&gt; &lt;p&gt;For new requests, the size of each buffer is fixed (MTU-based) but the number of buffers can be variable and OvS-DPDK will retry for a lower number of buffers if there is not enough memory for initial requests.&lt;/p&gt; &lt;p&gt;When DPDK cannot provide the requested memory to any one of the requests, it reports the following:&lt;/p&gt; &lt;pre&gt;|dpdk|ERR|RING: Cannot reserve memory&lt;/pre&gt; &lt;p&gt;While that may look serious, it&amp;#8217;s nothing to worry about because OvS handles this and simply retries for a lower amount. If however, the retries do not work then the following will be in the log:&lt;/p&gt; &lt;pre&gt;|netdev_dpdk|ERR|Failed to create memory pool for netdev dpdk0, with MTU 9000 on socket 0: Cannot allocate memory&lt;/pre&gt; &lt;p&gt;This case is an issue for functionality.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you were adding a port, it will not be usable.&lt;/li&gt; &lt;li&gt;If you were changing the MTU, the MTU change fails but the port will continue to operate with the previous MTU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How can you fix these errors? The general guide would be just to give OvS-DPDK more memory on the relevant NUMA node, or stick with a lower MTU.&lt;/p&gt; &lt;h2 id="starting-a-vm"&gt;Starting a VM&lt;/h2&gt; &lt;p&gt;It doesn&amp;#8217;t seem obvious why you would run out of memory when starting a VM, as opposed to when you are adding a vhost port for it (previous section). The key is vhost NUMA reallocation.&lt;/p&gt; &lt;p&gt;When a VM is started, DPDK checks the NUMA node of the memory shared from the guest. This may result in requesting a new pool of buffers from the same NUMA node. But of course, there might be no memory pre-allocated with &lt;code&gt;dpdk-socket-mem&lt;/code&gt; on that NUMA node, or else there might be insufficient memory left.&lt;/p&gt; &lt;p&gt;The log entry would be similar to the add port/change MTU cases:&lt;/p&gt; &lt;pre&gt;|netdev_dpdk|ERR|Failed to create memory pool for netdev vhost0, with MTU 1500 on socket 1: Cannot allocate memory&lt;/pre&gt; &lt;p&gt;The fix for this is having enough memory on the relevant NUMA node, or changing the libvirt/QEMU settings so VM memory is from a different NUMA node.&lt;/p&gt; &lt;h2 id="runtimeadding-a-portadding-queues"&gt;Runtime, Adding a Port, or Adding Queues&lt;/h2&gt; &lt;p&gt;Didn&amp;#8217;t we already cover &lt;em&gt;adding a port?&lt;/em&gt; Yes, we did; however, this section is for when we get a requested pool of buffers, but some time later that proves to be insufficient.&lt;/p&gt; &lt;p&gt;This might be because there are many ports and queues sharing a pool of buffers and by the time some buffers are reserved for Rx queues, some are in flight processing and some are waiting to be returned from Tx queues, so there just aren&amp;#8217;t enough buffers to go around.&lt;/p&gt; &lt;p&gt;For example, the log entries when this occurs while using a physical NIC could look like this:&lt;/p&gt; &lt;pre&gt;|dpdk|ERR|PMD: ixgbe_alloc_rx_queue_mbufs(): RX mbuf alloc failed queue_id=0 |dpdk|ERR|PMD: ixgbe_dev_rx_queue_start(): Could not alloc mbuf for queue:0 |dpdk|ERR|PMD: ixgbe_dev_start(): Unable to start rxtx queues |dpdk|ERR|PMD: ixgbe_dev_start(): failure in ixgbe_dev_start(): -1 |netdev_dpdk|ERR|Interface dpdk0 start error: Input/output error&lt;/pre&gt; &lt;p&gt;For vhost ports, buffers are not reserved but you could see at runtime that you cannot get a new buffer while polling vhost ports. The log entry could look like this:&lt;/p&gt; &lt;pre&gt;|dpdk(pmd91)|ERR|VHOST_DATA: Failed to allocate memory for mbuf.&lt;/pre&gt; &lt;p&gt;If all the ports are needed, the easiest way to resolve this is to reduce the numbers of Rx queues or reserved buffers for the physical NICs. This can be done with the following command:&lt;/p&gt; &lt;pre&gt;# ovs-vsctl set Interface dpdk0 options:n_rxq=4&lt;/pre&gt; &lt;p&gt;or with this command:&lt;/p&gt; &lt;pre&gt;# ovs-vsctl set Interface dpdk0 options:n_rxq_desc=1024&lt;/pre&gt; &lt;p&gt;Alternatively, memory could be increased to ensure that a large pool of buffers will be available (that is, avoiding retries for lower amounts) but that approach scales only so far.&lt;/p&gt; &lt;h2 id="further-debug"&gt;Further Debugging&lt;/h2&gt; &lt;p&gt;If you run out of memory, there will be an error message in the log. If you want further details about the pools of memory being allocated, reused,  and freed, you can turn on debug mode:&lt;/p&gt; &lt;pre&gt;# ovs-appctl vlog/set netdev_dpdk:console:dbg # ovs-appctl vlog/set netdev_dpdk:syslog:dbg # ovs-appctl vlog/set netdev_dpdk:file:dbg&lt;/pre&gt; &lt;p&gt;Allocated, reused,  and freed messages will look like this:&lt;/p&gt; &lt;pre&gt;|netdev_dpdk|DBG|Allocated "ovs_mp_2030_0_262144" mempool with 262144 mbufs |netdev_dpdk|DBG|Reusing mempool "ovs_mp_2030_0_262144" |netdev_dpdk|DBG|Freeing mempool "ovs_mp_2030_0_262144"&lt;/pre&gt; &lt;p&gt;The name of the pool of buffers (that is, &lt;code&gt;mempool&lt;/code&gt;) gives us some information:&lt;/p&gt; &lt;pre&gt;2030 : Padded size of the buffer (derived from MTU) 0 : NUMA node the memory is allocated from 262144 : Number of buffers in the pool&lt;/pre&gt; &lt;p&gt;There is also a command to show which &lt;code&gt;mempool&lt;/code&gt; a port is using, as well as lots of other details (not shown):&lt;/p&gt; &lt;pre&gt;# ovs-appctl netdev-dpdk/get-mempool-info dpdk0 mempool &amp;#60;ovs_mp_2030_0_262144&amp;#62;@0x7f35ff77ce40 ...&lt;/pre&gt; &lt;h2 id="wrap-up"&gt;Wrap-up&lt;/h2&gt; &lt;p&gt;If you have read to here, it probably means you&amp;#8217;ve hit an issue with OvS-DPDK. Sorry to hear that. Hopefully, after reading the above guide you&amp;#8217;ll be able to identify if the issue was due to running out of memory and you&amp;#8217;ll know how to fix it.&lt;/p&gt; &lt;p&gt;Some guidance on how much memory is required and how to configure OvS-DPDK for multi-NUMA (including &lt;code&gt;dpdk-socket-mem&lt;/code&gt;) can be found in the &lt;a href="https://developers.redhat.com/blog/2018/03/16/ovs-dpdk-hugepage-memory/"&gt;OVS-DPDK: How much memory&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/2017/06/28/ovs-dpdk-parameters-dealing-with-multi-numa/"&gt;OVS-DPDK: Multi-NUMA&lt;/a&gt; articles on this blog.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fdebugging-ovs-dpdk-memory-issues%2F&amp;#38;linkname=Debugging%20Memory%20Issues%20with%20Open%20vSwitch%20DPDK" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fdebugging-ovs-dpdk-memory-issues%2F&amp;#38;linkname=Debugging%20Memory%20Issues%20with%20Open%20vSwitch%20DPDK" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fdebugging-ovs-dpdk-memory-issues%2F&amp;#38;linkname=Debugging%20Memory%20Issues%20with%20Open%20vSwitch%20DPDK" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fdebugging-ovs-dpdk-memory-issues%2F&amp;#38;linkname=Debugging%20Memory%20Issues%20with%20Open%20vSwitch%20DPDK" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fdebugging-ovs-dpdk-memory-issues%2F&amp;#38;linkname=Debugging%20Memory%20Issues%20with%20Open%20vSwitch%20DPDK" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fdebugging-ovs-dpdk-memory-issues%2F&amp;#38;linkname=Debugging%20Memory%20Issues%20with%20Open%20vSwitch%20DPDK" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fdebugging-ovs-dpdk-memory-issues%2F&amp;#38;linkname=Debugging%20Memory%20Issues%20with%20Open%20vSwitch%20DPDK" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fdebugging-ovs-dpdk-memory-issues%2F&amp;#38;linkname=Debugging%20Memory%20Issues%20with%20Open%20vSwitch%20DPDK" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fdebugging-ovs-dpdk-memory-issues%2F&amp;#38;title=Debugging%20Memory%20Issues%20with%20Open%20vSwitch%20DPDK" data-a2a-url="https://developers.redhat.com/blog/2018/06/14/debugging-ovs-dpdk-memory-issues/" data-a2a-title="Debugging Memory Issues with Open vSwitch DPDK"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/14/debugging-ovs-dpdk-memory-issues/"&gt;Debugging Memory Issues with Open vSwitch DPDK&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ZoZS5zLebLI" height="1" width="1" alt=""/&gt;</content><summary>Introduction This article is about debugging out-of-memory issues with Open vSwitch with the Data Plane Development Kit (OvS-DPDK). It explains the situations in which you can run out of memory when using OvS-DPDK and it shows the log entries that are produced in those circumstances. It also shows some other log entries and commands for further debugging. When you finish reading this article, you ...</summary><dc:creator>Kevin Traynor</dc:creator><dc:date>2018-06-14T11:00:48Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/14/debugging-ovs-dpdk-memory-issues/</feedburner:origLink></entry><entry><title>Keycloak 4.0.0.Final Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rhgsic2mHfk/keycloak-400final-released.html" /><category term="feed_group_name_keycloak" scheme="searchisko:content:tags" /><category term="feed_name_keycloak" scheme="searchisko:content:tags" /><author><name>Stian Thorgersen</name></author><id>searchisko:content:id:jbossorg_blog-keycloak_4_0_0_final_released</id><updated>2018-06-14T07:38:19Z</updated><published>2018-06-14T07:38:00Z</published><content type="html">&lt;p&gt;To download the release go to the &lt;a href="http://www.keycloak.org/downloads"&gt;Keycloak homepage&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For details on what is included in the release check out the &lt;a href="https://www.keycloak.org/docs/latest/release_notes/index.html"&gt;Release notes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The full list of resolved issues is available in &lt;a href="https://issues.jboss.org/issues/?jql=project%20%3D%20keycloak%20and%20fixVersion%20%3D%204.0.0.Final"&gt;JIRA&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Before you upgrade remember to backup your database and check the &lt;a href="http://www.keycloak.org/docs/latest/upgrading/index.html"&gt;upgrade guide&lt;/a&gt; for anything that may have changed. &lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rhgsic2mHfk" height="1" width="1" alt=""/&gt;</content><summary>To download the release go to the Keycloak homepage. For details on what is included in the release check out the Release notes The full list of resolved issues is available in JIRA. Before you upgrade remember to backup your database and check the upgrade guide for anything that may have changed.</summary><dc:creator>Stian Thorgersen</dc:creator><dc:date>2018-06-14T07:38:00Z</dc:date><feedburner:origLink>http://blog.keycloak.org/2018/06/keycloak-400final-released.html</feedburner:origLink></entry><entry><title>Using the STOMP Protocol with Apache ActiveMQ Artemis Broker</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/b3AhZteGUJE/" /><category term="activemq" scheme="searchisko:content:tags" /><category term="AMQ" scheme="searchisko:content:tags" /><category term="artemis" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="integration" scheme="searchisko:content:tags" /><category term="jboss a-mq" scheme="searchisko:content:tags" /><category term="messaging" scheme="searchisko:content:tags" /><category term="Python" scheme="searchisko:content:tags" /><category term="Red Hat AMQ" scheme="searchisko:content:tags" /><category term="stomp" scheme="searchisko:content:tags" /><author><name>Chandra Shekhar Pandey</name></author><id>searchisko:content:id:jbossorg_blog-using_the_stomp_protocol_with_apache_activemq_artemis_broker</id><updated>2018-06-14T07:19:08Z</updated><published>2018-06-14T07:19:08Z</published><content type="html">&lt;p&gt;In this article, we will use a Python-based messaging client to connect and subscribe to a topic with a &lt;a href="https://developers.redhat.com/blog/2016/08/10/persistence-vs-durability-in-messaging/"&gt;durable&lt;/a&gt; subscription in the &lt;a href="https://activemq.apache.org/artemis/"&gt;Apache ActiveMQ Artemis&lt;/a&gt; broker. We will use the text-based &lt;a href="https://stomp.github.io/"&gt;STOMP&lt;/a&gt; protocol to connect and subscribe to the broker. STOMP clients can communicate with any STOMP message broker to provide messaging interoperability among many languages, platforms, and brokers.&lt;/p&gt; &lt;p&gt;If you need to brush up on the &lt;a href="https://developers.redhat.com/blog/2016/08/10/persistence-vs-durability-in-messaging/"&gt;difference between persistence and durability&lt;/a&gt; in messaging, check Mary Cochran&amp;#8217;s article on developers.redhat.com/blog.&lt;/p&gt; &lt;p&gt;A similar process can be used with &lt;a href="https://developers.redhat.com/products/amq/overview/"&gt;Red Hat AMQ 7&lt;/a&gt;. The broker in Red Hat AMQ 7 is based on the Apache ActiveMQ Artemis project. See the &lt;a href="https://developers.redhat.com/products/amq/overview/"&gt;overview&lt;/a&gt; on developers.redhat.com for more information.&lt;/p&gt; &lt;p&gt;&lt;span id="more-501697"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;&lt;strong&gt;Setting Up the Project&lt;/strong&gt;&lt;/h2&gt; &lt;p&gt;In the following example, we are using one client, both to publish and subscribe to a topic. You can find the code at my &lt;a href="https://github.com/1984shekhar/Artemis_POC/tree/master/python_stomp_example"&gt;personal GitHub repo&lt;/a&gt;. We have two &lt;a id="69bcdfb3c9c544f249bac6af7a20c063-a6f78788ddabb4df8495cae84d999a92e90a1359" class="js-navigation-open" title="receiver_queue.py" href="https://github.com/1984shekhar/Artemis_POC/blob/master/python_stomp_example/receiver_queue.py"&gt;receiver_queue.py&lt;/a&gt; and &lt;a id="fc379910fba7a36d1006069a40734294-e102b60425fc328abf23d49de36449ceaf0aaa37" class="js-navigation-open" title="receiver_topic.py" href="https://github.com/1984shekhar/Artemis_POC/blob/master/python_stomp_example/receiver_topic.py"&gt;receiver_topic.py&lt;/a&gt; Python messaging clients. While &lt;code&gt;receiver_queue.py&lt;/code&gt; is a Python client based on the STOMP protocol for point-to-point (queue) connection to the broker, &lt;code&gt;receiver_topic.py&lt;/code&gt; is a Python client based on the STOMP protocol for durable subscription against a topic to the broker.&lt;/p&gt; &lt;p&gt;Here is the code:&lt;/p&gt; &lt;pre&gt;import time import sys import stomp class MyListener(stomp.ConnectionListener): def on_error(self, headers, message): print('received an error "%s"' % message) def on_message(self, headers, message): print('received a message "%s"' % message) hosts = [('localhost', 61616)] conn = stomp.Connection(host_and_ports=hosts) conn.set_listener('', MyListener()) conn.start() conn.connect('admin', 'admin', wait=True,headers = {'client-id': 'clientname'} ) conn.subscribe(destination='A.B.C.D', id=1, ack='auto',headers = {'subscription-type': 'MULTICAST','durable-subscription-name':'someValue'}) conn.send(body=' '.join(sys.argv[1:]), destination='A.B.C.D') time.sleep(2) conn.disconnect() &lt;/pre&gt; &lt;p&gt;The following are tasks performed by this code:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;To receive messages from the messaging system, we need to set up a listener on a connection, and then later subscribe to the destination.&lt;/li&gt; &lt;li&gt;We are establishing a connection to the broker available locally on port 61616. The first parameter to a &lt;code&gt;Connection&lt;/code&gt; is &lt;code&gt;host_and_ports&lt;/code&gt;. This contains an IP address and the port where the message broker is listening for STOMP connections.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;start&lt;/code&gt; method creates a socket connection to the broker.&lt;/li&gt; &lt;li&gt;Then we use the &lt;code&gt;connect&lt;/code&gt; method with credentials to access the broker and we use the &lt;code&gt;headers&lt;/code&gt; &lt;code&gt;client-id&lt;/code&gt; to ensure that the subscription that is created is durable.&lt;/li&gt; &lt;li&gt;Once a connection is established to the broker with &lt;code&gt;subscribe&lt;/code&gt; method, we are subscribing to destination &lt;code&gt;A.B.C.D&lt;/code&gt; using acknowledgment mode &lt;code&gt;auto&lt;/code&gt;. Also, we must provide the &lt;code&gt;headers&lt;/code&gt; subscription-type as &lt;code&gt;MULTICAST&lt;/code&gt; and &lt;code&gt;durable-subscription-name&lt;/code&gt; as some text value.&lt;/li&gt; &lt;li&gt;To create a durable subscription, the &lt;code&gt;client-id&lt;/code&gt; header must be set on the &lt;code&gt;CONNECT&lt;/code&gt; frame and the &lt;code&gt;durable-subscription-name&lt;/code&gt; must be set on the &lt;code&gt;SUBSCRIBE&lt;/code&gt; frame. The combination of these two headers will form the identity of the durable subscription.&lt;/li&gt; &lt;li&gt;After a connection is established to the broker, we can use the &lt;code&gt;send&lt;/code&gt; method to send/produce messages to the destination A.B.C.D. Here the first argument is to accept the text/string value from the command line, and the second argument is destination name or topic name.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;strong&gt;How to Execute the Python Client&lt;/strong&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Make sure the  Apache ActiveMQ Artemis broker is configured to support the STOMP protocol. By default, port 61616 is configured to support almost all messaging protocols.&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;&amp;#60;acceptor name="artemis"&amp;#62;tcp://0.0.0.0:61616?tcpSendBufferSize=1048576;tcpReceiveBufferSize=1048576;protocols=CORE,AMQP,STOMP,HORNETQ,MQTT,OPENWIRE;useEpoll=true;amqpCredits=1000;amqpLowCredits=300&amp;#60;/acceptor&amp;#62; &lt;/pre&gt; &lt;ul&gt; &lt;li&gt;To run the client using the STOMP protocol, we first need the &lt;code&gt;stomp&lt;/code&gt; module so that components of the STOMP API, such as &lt;code&gt;connect&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;,  &lt;code&gt;send&lt;/code&gt;, &lt;code&gt;subscribe&lt;/code&gt;, and &lt;code&gt;disconnect&lt;/code&gt;, are available. So install the &lt;code&gt;stomp&lt;/code&gt; module first.&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;pip install stomp.py &lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Once the &lt;code&gt;stomp&lt;/code&gt; module is installed, we can easily run the client in the following way:&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;[cpandey@vm254-231 python_stomp_example]$ python receiver_topic.py "Hello World" received a message "Hello World" [cpandey@vm254-231 python_stomp_example]$ &lt;/pre&gt; &lt;ul&gt; &lt;li&gt;We can check the results using the following commands from the Apache ActiveMQ Artemis broker:&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;[cpandey@vm254-231 bin]$ ./artemis address show A.B.C.D DLQ [cpandey@vm254-231 bin]$ ./artemis queue stat --user admin --password admin --url tcp://localhost:61616 OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N |NAME |ADDRESS |CONSUMER_COUNT |MESSAGE_COUNT |MESSAGES_ADDED |DELIVERING_COUNT |MESSAGES_ACKED | |DLQ |DLQ |0 |0 |0 |0 |0 | |ExpiryQueue |ExpiryQueue |0 |0 |0 |0 |0 | |clientname.someValue |A.B.C.D |0 |0 |1 |0 |1 | [cpandey@vm254-231 bin]$ &lt;/pre&gt; &lt;p&gt;Note: A.B.C.D is the &lt;code&gt;Address&lt;/code&gt; created and the durable subscription is created as queue &lt;code&gt;clientname.someValue&lt;/code&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If we read the network dumps using Wireshark, the following is the complete stream:&lt;/li&gt; &lt;/ul&gt; &lt;pre&gt;STOMP accept-version:1.1 client-id:clientname login:admin passcode:admin .CONNECTED version:1.1 session:4c98c896 server:ActiveMQ-Artemis/2.4.0.amq-711002-redhat-1 ActiveMQ Artemis Messaging Engine . SUBSCRIBE ack:auto destination:A.B.C.D durable-subscription-name:someValue id:1 subscription-type:MULTICAST .SEND content-length:4 destination:A.B.C.D abcd.MESSAGE subscription:1 content-length:4 message-id:30 destination:A.B.C.D expires:0 redelivered:false priority:4 persistent:false timestamp:1528858440363 abcd. DISCONNECT receipt:6a8bc1fd-0c8b-4e13-871f-fbc9c8c4df9d .RECEIPT receipt-id:6a8bc1fd-0c8b-4e13-871f-fbc9c8c4df9d &lt;/pre&gt; &lt;p&gt;That&amp;#8217;s it. I hope this helps you to have a basic understanding of using the STOMP protocol with the Apache ActiveMQ Artemis or Red Hat AMQ 7.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fstomp-with-activemq-artemis-python%2F&amp;#38;linkname=Using%20the%20STOMP%20Protocol%20with%20Apache%20ActiveMQ%20Artemis%20Broker" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fstomp-with-activemq-artemis-python%2F&amp;#38;linkname=Using%20the%20STOMP%20Protocol%20with%20Apache%20ActiveMQ%20Artemis%20Broker" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fstomp-with-activemq-artemis-python%2F&amp;#38;linkname=Using%20the%20STOMP%20Protocol%20with%20Apache%20ActiveMQ%20Artemis%20Broker" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fstomp-with-activemq-artemis-python%2F&amp;#38;linkname=Using%20the%20STOMP%20Protocol%20with%20Apache%20ActiveMQ%20Artemis%20Broker" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fstomp-with-activemq-artemis-python%2F&amp;#38;linkname=Using%20the%20STOMP%20Protocol%20with%20Apache%20ActiveMQ%20Artemis%20Broker" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fstomp-with-activemq-artemis-python%2F&amp;#38;linkname=Using%20the%20STOMP%20Protocol%20with%20Apache%20ActiveMQ%20Artemis%20Broker" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fstomp-with-activemq-artemis-python%2F&amp;#38;linkname=Using%20the%20STOMP%20Protocol%20with%20Apache%20ActiveMQ%20Artemis%20Broker" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fstomp-with-activemq-artemis-python%2F&amp;#38;linkname=Using%20the%20STOMP%20Protocol%20with%20Apache%20ActiveMQ%20Artemis%20Broker" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F14%2Fstomp-with-activemq-artemis-python%2F&amp;#38;title=Using%20the%20STOMP%20Protocol%20with%20Apache%20ActiveMQ%20Artemis%20Broker" data-a2a-url="https://developers.redhat.com/blog/2018/06/14/stomp-with-activemq-artemis-python/" data-a2a-title="Using the STOMP Protocol with Apache ActiveMQ Artemis Broker"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/14/stomp-with-activemq-artemis-python/"&gt;Using the STOMP Protocol with Apache ActiveMQ Artemis Broker&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/b3AhZteGUJE" height="1" width="1" alt=""/&gt;</content><summary>In this article, we will use a Python-based messaging client to connect and subscribe to a topic with a durable subscription in the Apache ActiveMQ Artemis broker. We will use the text-based STOMP protocol to connect and subscribe to the broker. STOMP clients can communicate with any STOMP message broker to provide messaging interoperability among many languages, platforms, and brokers. If you nee...</summary><dc:creator>Chandra Shekhar Pandey</dc:creator><dc:date>2018-06-14T07:19:08Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/14/stomp-with-activemq-artemis-python/</feedburner:origLink></entry></feed>
